---
title: "NRES 776 Lecture 19"
subtitle: "Multiple regression with GLM"
author: Sunny Tseng
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    #css: styles.css
    #footer: <https://quarto.org>
    theme: [default, styles.scss]
    #smaller: true
    #scrollable: true
    incremental: true
    embed-resources: true
    width: 1200
    fontsize: 1.7em
editor: visual
---

```{r setup}
library(tidyverse)
library(here)
library(lmtest)

binomial_data <- read_csv(here("Lectures", "Lecture 19 - multiple regression with GLM", "data", "isolation.csv"))

binomial_data <- binomial_data %>% 
  mutate(isolation = cut(isolation, breaks = c(2, 5, 7, 10), 
                         labels = c("low", "median", "high"))) %>%
  mutate(incidence = as_factor(incidence)) 

```

## Our schedule today

-   Announcement (3 min)

    -   zoom recording

    -   

-   Wrap up (5 min)

## UCB Admission data set

Aggregate data on applicants to graduate school at Berkeley for the six largest departments in 1973 classified by admission and sex.

```{r, echo = TRUE}
data("UCBAdmissions")
```

![](images/Screenshot%202023-11-07%20091754.png){fig-align="center" width="669"}

## Exploratory data visualization

```{r, echo = TRUE}
apply(UCBAdmissions, c(1, 2), sum)
mosaicplot(apply(UCBAdmissions, c(1, 2), sum),
           main = "Student admissions at UC Berkeley")
```

## Model formulation (glm_1)

$$
logit(admission) = \beta_0 + \beta_1 genderF_i
$$

```{r, echo = TRUE}
glm_1 <- glm(formula = admission ~ gender, 
             data = UCBAdmissions, 
             family = "binomial")
```

## Model goodness of fit

## Coef. interpretation

For male applicants

For female applicants

## Conclusion

Evidence of sex bias in admission practices

## Model formulation (glm_2)

## Model goodness of fit

## Coef. interpretation

## Comparision between two models

## What is actually going on

```{r, echo = TRUE}
opar <- par(mfrow = c(2, 3), oma = c(0, 0, 2, 0))
for(i in 1:6)
  mosaicplot(UCBAdmissions[,,i],
    xlab = "Admit", ylab = "Sex",
    main = paste("Department", LETTERS[i]))
mtext(expression(bold("Student admissions at UC Berkeley")),
      outer = TRUE, cex = 1.5)
par(opar)
```

## Simpson's Paradox

> A phenomenon in probability and statistics in which a trend appears in several groups of data but disappears or reverses when the groups are combined.

Which means, in a easier language, missing of important variable in a model.

![](https://paulvanderlaken.files.wordpress.com/2017/09/simpsonsparadox.png?w=1200){fig-align="center"}

## Overview of Binomial data

### Applications

-   Predict the winner of a sport game (team A or team B)

-   Predict animal behaviour (eat or not eat)

-   Evaluate business decisions (invest or not)

### Data requirement

-   Binary data (0 or 1)

-   Survival data (alive, dead)

-   Choice or behaviour (yes or no)

-   Result (pass or fail)

### In short

-   Use binomial regression when $y \sim Binomial(p)$

## Binomial distribution

::: columns
::: {.column width="50%"}
### Bernoulli distribution

-   Describe a variable which takes the value 1 with probability $p$ and the value 0 with probability $1-p$

-   The probability of front/back by flipping one coin.

-   $f(k;p) = p^k(1-p)^{1-k}$ for $k \in \{0, 1\}$

![](https://d138zd1ktt9iqe.cloudfront.net/media/seo_landing_files/bernoulli-distribution-graph-1634631289.png){fig-align="center" width="284"}
:::

::: {.column width="50%"}
### Binomial distribution

-   The \# of successes in a sequence of $n$ independant experiments.

-   The probability of having $k$ coins facing up after tossing $n$ coins.

-   $f(k, n, p) = {n \choose x}p^k (1-p)^{1-k}$

-   Bernoulli distribution is a special case of Binomial distribution when $n =1$.

![](https://datasciencelk.com/wp-content/uploads/2020/03/Binomial_Distribution-1536x1466.jpg.webp){fig-align="center" width="223"}
:::
:::

## Overview of Binomial regression

1.  Systematic component

$$ \eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} $$

2.  Link function $g()$ --\> most often **logit** (log odds), another common one is "probit"

$$ \eta_i = g(p_i) = logit(p_i) = log(\frac{p_i}{1-p_i}) $$

3.  Random component

$$ var(y_i) = n_ip_i(1-p_i) $$

## Overview of Binomial regression (con'd)

### Equation for binomial regression

$$ logit(\mu_i) = logit(p_i) = log(\frac{p_i}{1-p_i})= \beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi} $$

### Back transformation to get probability

$$
\mu_i = p_i = logit^{-1}(\beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi}) = \frac{1}{1+ exp(\beta_0 + \beta_1 x_{1i} + ...)}
$$

### Or, back transformation to get odds

$$
Odds = \frac{p_i}{1-p_i} = exp(\beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi})
$$

## Probability, Odds, Odds Ratio (OR)

### From probability to Odds

$$
Odds = \frac{p_i}{1-p_i} = exp(\beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi})
$$

-   The probability that the event will occur divided by the probability that the event will not occur.

-   Odds increases as the probability increases.

::: columns
::: {.column width="50%"}
![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*8ix_A7GUKH9AsZxouYg-uw.png){fig-align="center" width="420"}
:::

::: {.column width="50%"}
![](images/Capture-01.PNG){fig-align="center" width="423"}
:::
:::

## Probability, Odds, Odds Ratio (OR) (con'd)

::: columns
::: {.column width="50%"}
### From Odds to Odds Ratio (OR)

-   Odd ratios compare the odds of an event in two different groups
-   $OR = \frac{Odds(A)}{Odds(B)}= \frac{p_A/(1-p_A)}{p_B/(1-p_B)}$
-   $OR = 1$, no difference between groups
-   $OR < 1$, treatment decreases odds
-   $OR > 1$, treatment increases odds
:::

::: {.column width="50%"}
![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Odds_ratio_map.svg/1280px-Odds_ratio_map.svg.png){fig-align="center" width="620"}
:::
:::

::: aside
Note: the figure is showing log(OR), but the idea is the same. OR, or log(OR) is higher when the event has higher probability compared to another one. Figure from Wikipedia.
:::

## Binomial regression in R

-   By default `family = binomial(link = "logit")`
-   The variance for this distribution is `variance = "mu(1-mu)"`, and you cannot change it from the default.

```{r}
bird_incidence <- binomial_data %>%   
  select(incidence, area)
```

```{r, echo = TRUE}
glm_binomial <- glm(formula = incidence ~ area, 
                    data = bird_incidence,                     
                    family = "binomial")  
glm_binomial %>% summary
```

## Long vs wide format

A survey was done on 50 islands for the incidence of a bird species Grasshopper Warbler. Researchers want to know whether the incidence is related to the area and/or the isolation level of the islands.

::: columns
::: {.column width="50%"}
### Long format

-   Each row is an individual observation

-   Binary output

```{r}
data_long <- binomial_data
data_long
```
:::

::: {.column width="50%"}
### Wide format

-   Each row is a group observation

-   Ratio, or proportion

```{r}
data_wide <- binomial_data %>%
  group_by(isolation) %>%
  summarize(presence = sum(incidence == 1),
            absence = sum(incidence == 0),
            total = n(),
            proportion = presence/total,
            mean_area = mean(area)) %>%
  relocate(isolation, .after = mean_area)

data_wide
```
:::
:::

## Long vs wide format (con'd)

::: columns
::: {.column width="50%"}
### Long format

-   Directly put $y$ as the Binary output

```{r, echo = TRUE}
glm_long <- glm(formula = incidence ~ isolation,
                data = data_long,
                family = "binomial")

glm_long %>% coef()
```
:::

::: {.column width="50%"}
### Wide format

-   Use the number of presence and absence

```{r, echo = TRUE}
glm_wide <- glm(formula = 
                  cbind(presence, absence) ~ isolation,
                data = data_wide,
                family = "binomial")

glm_wide %>% coef()
```

-   Or, give R the proportion and the total count

```{r, echo = TRUE}
glm_wide <- glm(formula = proportion ~ isolation,
                weights = total,
                data = data_wide, 
                family = "binomial")

glm_wide %>% coef()
```
:::
:::

## Long vs wide format (con'd)

### What's the same

-   The raw data used

-   The "direction" of coefficients

### What's the difference

-   The coefficient values (Note: These would be the same if the data is balanced)

### When to use what

-   What's your raw data structure?

-   Which variables you have? individual or group?

-   Do you want to make inference to group or individual?

    -   e.g., probability of eggs hatching in a nest -\> nest success? or success of individual eggs?

## Research question (1 categorical x)

A survey was done on 50 islands for the incidence of a bird species Grasshopper Warbler. Researchers want to know whether the incidence is related to the area and/or the isolation level of the islands.

::: columns
::: {.column width="50%"}
### Use long format as input

```{r}
data_long
```
:::

::: {.column width="50%"}
### Take a look at the group means

```{r}
data_wide <- binomial_data %>%
  group_by(isolation) %>%
  summarize(presence = sum(incidence == 1),
            absence = sum(incidence == 0),
            total = n(),
            proportion = presence/total,
            mean_area = mean(area)) %>%
  select(proportion, total, isolation)

data_wide
```
:::
:::

## Model formulation

$$
logit(p_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
$$

```{r, echo = TRUE}
incidence_binom <- glm(formula = incidence ~ isolation, 
                       data = data_long, 
                       family = "binomial")

incidence_binom %>% summary
```

## Coef. interpretation

-   For Low isolation:

$$
logit(p_L) = log(\frac{p_L}{1-p_L})= \beta_0
$$

$$
\frac{p_L}{1-p_L} = Odd(p_L) = exp(\beta_0) = exp(20.57) = 8.5 * 10^8
$$

. . .

-   For Median isolation (use odd ratio):

$$
logit(p_M) = \beta_0 + \beta_1
$$

$$
logit(p_M) - logit(p_L) = \beta_1 = log(\frac{Odd(p_M)}{Odd(p_L)})
$$

$$
\frac{Odd(p_M)}{Odd(p_L)} = exp(\beta_1) = exp(-20.01) = 2.04 * 10^{-9} 
$$

## Coef. interpretation (con'd)

-   For High isolation (use odd ratio):

$$
logit(p_M) = \beta_0 + \beta_2
$$

$$
\frac{Odd(p_H)}{Odd(p_L)} = exp(\beta_2) = exp(-41.13) = 1.36 * 10^{-18} 
$$

## Output interpretation

### Odd ratio (OR)

-   $OR = 1$, no difference between groups
-   $OR < 1$, treatment decreases odds
-   $OR > 1$, treatment increases odds

### R output

-   **Intercept (20.57)**: The odd of the bird being present on low isolation island is $exp(20.57)$

-   **isolationmedian (-20.01)**: The odd ratio of bird being present on median isolation island compared to low isolation island is $exp(-20.01)$

-   **isolationhigh (-41.13)**: The odd ratio of bird being present on median isolation island compared to low isolation island is $exp(-41.13)$

-   **Dispersion parameter (1)**: wonderful. Need to consider other methods if dispersion larger than 1 (over-dispersion) or smaller than 1 (under-dispersion)

-   **AIC (34.841)**: Can be used to compare the goodness of fit between models

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
incidence_binom_null <- glm(formula = incidence ~ 1, 
                       data = data_long, 
                       family = "binomial")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Use `lrtest()` function in the `lmtest` package

```{r, echo = TRUE}
lrtest(incidence_binom, incidence_binom_null)
```

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
incidence_binom_null <- glm(formula = incidence ~ 1, 
                       data = data_long, 
                       family = "binomial")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Or, Use `anova()` and specify `test = "Chisq"`

```{r, echo = TRUE}
anova(incidence_binom, incidence_binom_null, test = "Chisq")
```

## Predictor significance: Likelihood ratio test

-   Test whether adding one more variable `area` could increase the model performance

$H_0$: The full model performance is the same as a reduced model (whichever model have fewer predictors)

$H_1$: The full model performance is significantly different comparing to a reduced model

. . .

```{r, echo = TRUE}
incidence_binom_add <- glm(formula = incidence ~ isolation + area, 
                       data = data_long, 
                       family = "binomial")

lrtest(incidence_binom, incidence_binom_add)
```

## Model prediction

-   Use `predict()` and specify `type = "response"` to get back transformed $y_i$

```{r, echo = TRUE}
incidence_p <- data_long %>%
  mutate(incidence_p = predict(incidence_binom_add, type = "response"))

incidence_p
```

## Model prediction

-   Or, use `fitted()`, which provides back transformed $y_i$ by default

```{r, echo = TRUE}
incidence_p <- data_long %>%
  mutate(incidence_p = fitted(incidence_binom_add))

incidence_p
```

## Model visualization

```{r, echo = TRUE}
ggplot(aes(x = isolation, y = incidence_p), data = incidence_p) + 
  geom_boxplot() +
  labs(y = "Probability of incidence", x = "Isolation level of island")
```

## Research question (1 continuous x)

A survey was done on 50 islands for the incidence of a bird species Grasshopper Warbler. Researchers want to know whether the incidence is related to the area and/or the isolation level of the islands.

::: columns
::: {.column width="50%"}
### Use long format as input

```{r}
data_long
```
:::

::: {.column width="50%"}
### Take a look at the relationship

```{r}
data_wide <- binomial_data %>%   
  group_by(isolation) %>%   
  summarize(presence = sum(incidence == 1),
            absence = sum(incidence == 0),
            total = n(),             
            proportion = presence/total, 
            mean_area = mean(area)) %>%   
  select(proportion, total, mean_area)  
data_wide
```
:::
:::

## Model formulation

$$ logit(p_i) = \beta_0 + \beta_1 x_{1i} $$

```{r, echo = TRUE}
incidence_binom <- glm(formula = incidence ~ area,
                       data = data_long,
                       family = "binomial")

incidence_binom %>% summary
```

## Coef. interpretation

-   For island with area as 0 (baseline):

$$ logit(p_i) = log(\frac{p_i}{1-p_i})= \beta_0 $$

$$ \frac{p_i}{1-p_i} = Odd(p_i) = exp(\beta_0) = exp(-2.15) = 0.11 $$

-   The odd for the intercept is not often interpreted by itself.

## Coef. interpretation (con'd)

-   For island with non-0 area:

$$ logit(p_i) = \beta_0 + \beta_1 x_{1i}$$

$$ logit(p_i') = \beta_0 + \beta_1 (x_{1i} + 1)$$

$$ logit(p_i') - logit(p_i) = \beta_1 = log(\frac{Odd(p_i')}{Odd(p_i)}) $$

$$ \frac{Odd(p_i')}{Odd(p_i)} = exp(\beta_1) = exp(0.62) = 1.87  $$

-   For one unit increase in `area`, the odds of the bird species being present increase by a factor of 1.87.

## Output interpretation

### Odd ratio (OR)

-   $OR = 1$, no difference between groups
-   $OR < 1$, treatment decreases odds
-   $OR > 1$, treatment increases odds

### R output

-   **Intercept (-2.15)**: The odd of the bird being present on 0 area island is $exp(-2.15)$

-   **area (0.62)**: For one unit increase in `area`, the odds of the bird species being present increase by a factor of $exp(0.62)$

-   **Dispersion parameter (1)**: wonderful. Need to consider other methods if dispersion larger than 1 (over-dispersion) or smaller than 1 (under-dispersion)

-   **AIC (54.172)**: Can be used to compare the goodness of fit between models

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
incidence_binom_null <- glm(formula = incidence ~ 1,
                            data = data_long,
                            family = "binomial")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Use `lrtest()` function in the `lmtest` package

```{r, echo = TRUE}
lrtest(incidence_binom, incidence_binom_null)
```

## Model prediction

-   Use `predict()` and specify `type = "response"` to get back transformed $y_i$

```{r, echo = TRUE}
incidence_p <- data_long %>%   
  mutate(incidence_p = predict(incidence_binom_add, type = "response")) 
incidence_p
```

## Model visualization

```{r, echo = TRUE}
data_long %>%
  mutate(incidence = incidence %>% as.numeric() - 1) %>%
  ggplot(aes(x = area, y = incidence), data = .) +
  geom_jitter(width = 0, height = 0.05) +
  geom_smooth(method =  "glm", 
              method.args = list(family = "binomial"),
              se = FALSE) +
  labs(y = "Probability of incidence", x = "Area of an island")
  
```

## What we learned today

-   What is Binomial distribution

-   We can use long data or wide data to fit binomial regression

-   Binomial regression coefficient interpretation

-   Goodness of fit

-   Visualization

## Wrap up

### Before we meet again

-   Vote for the lab 10 topic (<https://forms.gle/gjqQsMFPdmp86yX98>)

### Next time

-   Next Thursday morning 8 am lab, virtual on zoom
