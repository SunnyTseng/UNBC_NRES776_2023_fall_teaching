---
title: "NRES 776 Lecture 16"
subtitle: "Intro to GLM"
author: Sunny Tseng
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    #css: styles.css
    #footer: <https://quarto.org>
    theme: [default, styles.scss]
    #smaller: true
    #scrollable: true
    incremental: true
    embed-resources: true
    width: 1200
    fontsize: 1.7em
editor: visual
---

```{r setup}
library(palmerpenguins)
library(tidyverse)
```

## Our schedule today

-   Announcement (5 min)

-   Happy Halloween (8 min)

-   Review of LM and GLM (12 min)

-   Poisson regression (20 min)

-   Wrap up (5 min)

### What we will cover about GLM

1.  Review and limit of linear regression. Poisson regression.
2.  Logistic (bionomial) regression
3.  Interpreting and plotting GLM
4.  Multiple regression with GLM

## Announcement

-   Project 1 marking is till ongoing
-   Discussion presentation on Generalized Linear Models (Nov.9 and Nov.16): one group presents one publication

![](https://cdn.myportfolio.com/45214904-6a61-4e23-98d6-b140f8654a40/8b8ee9c3-c9db-458b-869a-35eda4913056.png?h=ffc16eac0bd6927d676b62824aba3b20){fig-align="center" width="675"}

::: aside
Artwork by @allison_horst
:::

## Happy Halloween

Want to try some trivia for practice?

# Generalized Linear Model (GLM)

## Power horse of statistics

-   t-test: comparison between means of two groups

-   ANOVA: comparison between means of multiple groups

-   Correlation: how two variables are correlated

    -   Pearson's correlation coefficient: linear relationship

    -   Spearman's correlation coefficient: monotonic relationship

-   Linear models (ordinary least square, OLS): $\beta_0$ shows the baseline effect; $\beta_1$ shows the effect, slope, of predictor $x$ on response $y$

    -   continuous $y$ and categorical/continuous $x$

    -   multiple $x$

    -   transformation of $y$ or $x$: log, quadratic, square root, etc.

-   What more can we ask for?

## Assumptions of **linear** models

$$
\mu_i = y_i - \epsilon_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} +... + \beta_p x_{pi}
$$

$$
\epsilon_i \sim N(0, \sigma_{\epsilon}^2), iid
$$

$$
var(y_i) = var(\epsilon_i) = \sigma_\epsilon^2 
$$

-   **L**inear

-   **I**ndependent

-   **N**ormality

-   **E**qual variance

## Generalized linear model

> is a statistical modelling technique formulated by John Nelder and Robert Wedderburn in 1972. It allows the response variable $y$ to have an error distribution other than a normal distribution.The models include Linear Regression, Logistic Regression, and Poisson Regression.

-   Generalized: GLM can accommodates other error structures (e.g., Poisson, Binomial) in addition to Normal

-   Linear: The parameters, coefficients (i.e., $\beta$) are linearly combined

::: aside
https://www.youtube.com/watch?app=desktop&v=ddCO2714W-o
:::

## Linear models vs Generalized linear models

::: columns
::: {.column width="50%"}
Linear Models (1809)

-   $y_i \sim N(\mu_i, \sigma^2)$

-   $\mu_i = x_i'\beta$

-   Least square, or maximum likehood for parameter estimation

-   Use `lm()` function in R
:::

::: {.column width="50%"}
Generalized linear models (1972)

-   $y_i \sim$ exponential family distribution

-   $g(\mu_i) = x_i'\beta$, $g()$ can be logit, inverse, log, exponential, etc

-   Only maximum likelihood

-   Use `glm()` function in R
:::
:::

> Linear models is a special case of generalized linear models, where the error distribution follows normal distribution

## Linear model, a special case of GLM

Fit a linear model using `lm()`

```{r, echo = TRUE}
lm(formula = Sepal.Length ~ Species, data = iris)
```

Fit a linear model using `glm()`, specify `family = "gaussian"`

```{r}
glm(formula = Sepal.Length ~ Species, data = iris, family = "gaussian")
```

## Three components for a GLM

1.  Systematic component, or linear predictor: linear combinations of regression coefficients

$$
\eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi}
$$

2.  Link function $g()$: how the predicted response relates to the systematic component

$$
\eta_i = g(\mu_i)
$$

3.  Random component, or variance function: refers to the probability distribution, from the family of distributions of the response variable

$$
var(y_i) = \phi V(\mu_i)
$$

## Three components for a GLM

-   We can use **back transformation** to get $y_i$:

    $$
    \eta_i = g(\mu_i) = g(y_i - \epsilon_i) = x_i'\beta 
    $$

    $$
    y_i = g^{-1}(x_i' \beta) + \epsilon_i
    $$

-   $\phi$ is a constant, called the scale parameter

-   $V$ is a variance function (variance as a function of the means)

## Commonly used GLM: Normal

1.  Systematic component

$$ \eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} $$

2.  Link function $g()$ --\> **identity**

$$ \eta_i = g(\mu_i) = \mu_i $$

3.  Random component --\> **constant**

$$ var(y_i) = \sigma^2 $$

-   Back transform:

    $$
    y_i = g^{-1}(x_i' \beta) + \epsilon_i
    $$

    $$
    y_i = x_i'\beta + \epsilon_i
    $$

## Commonly used GLM: Binomial

# Poisson Regression

## Wrap up

### Before we meet again

-   Review the two lectures (important)

-   Review and practice with the R coding (also important)

-   Maybe use what we learned to explore your own data

### Next time

-   Thursday lab (Nov.2, 8am) is in person with Lisa
-   
