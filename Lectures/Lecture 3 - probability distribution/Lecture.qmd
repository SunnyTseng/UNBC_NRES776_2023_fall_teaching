---
title: "NRES 776 Lecture 3"
subtitle: "Probability distributions"
author: Sunny Tseng
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    css: styles.css
    #footer: <https://quarto.org>
    theme: [default, styles.scss]
    smaller: true
    #scrollable: true
    incremental: true
    embed-resources: true
editor: visual
---

## Our schedule today

-   Announcement (5 min)

-   Random variables (15 min)

-   Probability distribution (25 min)

-   Wrap up (5 min)

## Announcement

-   Let's take this lecture easy...

![](images/paste-22B16D7F.png){fig-align="center" width="608"}

::: aside
Artwork by @allison_horst
:::

# Understand your variable

## Random variable

> Random variable: a variable whose value has probability associated to each value of the set. A easier way to understnd: the value that you are interested in.

. . .

Always use capital letters like $X$, $Y$, or $Z$ to represent a random variable

. . .

e.g., deer height, deer weight, number of trees in a plot

e.g., age of a student in the class $X$

. . .

::: columns
::: {.column width="50%"}
```{r, eval = TRUE, echo = FALSE}
# load library rcompanion
library(rcompanion)
  
# create data vector
x= c(rnorm(n = 30, mean = 26, sd = 1.2))
  
# draw plot
plotNormalHistogram(x, prob = TRUE,
                      main = "Age of our class",
                      length = 1000 )
```
:::

::: {.column width="50%"}
<div>

The probability of $X$ being a certain value:

$$
P(X = 26) = 0.3
$$

</div>
:::
:::

. . .

**Random variable can be either continuous or categorical.**

## Continuous vs categorical

-   Continuous variable: can have infinite values within possible range `numeric()`

-   Categorical (discrete) variable: can only exist at limited values

![](images/paste-8AAA3711.png){fig-align="center" width="634"}

::: aside
Artwork by @allison_horst
:::

## Within categorical variable, there are:

-   Nominal: unordered descriptions `character()`

-   Ordinal: ordered descriptions `factor()`

-   Binary: only 2 mutually exclusive outcomes `logical()`

![](images/paste-8F12C5B9.png){fig-align="center" width="622"}

::: aside
Artwork by @allison_horst
:::

# Probability distribution

The common distributions that the random variables can follow

. . .

This part is especially important as a foundation of hypothesis test (you will know why in a week)

## Normal distribution

> Or Gaussian distribution. A bell-shaped curve, the tails on both side are continuous to infinity. Symmetric.

. . .

::: columns
::: {.column width="40%"}
$$ f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$

$$ X \sim N(\mu, \sigma^2) $$

-   $X$ follows a normal distribution

-   Parameters $\mu$ and $\sigma$

-   **The area under the curve represent the probability**

-   **Sum of the area equal to 1 (100%)**
:::

::: {.column width="60%"}
```{r}
library(tidyverse)
p <- data_frame(x = c(16, 36)) %>% 
    ggplot(aes(x = x)) 
#> Warning: `data_frame()` is deprecated, use `tibble()`.
#> This warning is displayed once per session.

p + geom_vline(xintercept = 26, linetype = "dashed", alpha = 0.4) +
    stat_function(fun = dnorm, args = list(mean = 26, sd = 3), color = "#84CA72", size = 1.5) +
    stat_function(fun = dnorm, args = list(mean = 26, sd = 6), color = "#236ED3", size = 1.5) +
    ggtitle("Normal Distribution, mean = 26, sd = 3 (green) or 6 (blue)") +
    xlab("X") +
    ylab("f(X)") +
    theme_classic()
```
:::
:::

## Z-distribution

> A special case of Normal distribution when $\mu$ is 0 and $\sigma$ is 1, that is $N(0, 1^2)$. To transform ANY $N(\mu, \sigma^2)$ to $N(0, 1^2)$:

. . .

::: columns
::: {.column width="50%"}
$$
x_i^\prime = \frac{x_i - \mu}{\sigma}
$$

-   No parameter needed

```{=html}
<!-- -->
```
-   **Standardized normal is easier for us to find the corresponded probability of a value**
:::

::: {.column width="50%"}
```{r}
p <- data_frame(x = c(-4,4)) %>% 
    ggplot(aes(x = x)) 
#> Warning: `data_frame()` is deprecated, use `tibble()`.
#> This warning is displayed once per session.

p + geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.4) +
  geom_vline(xintercept = 1, linetype = "solid", alpha = 0.4) + geom_vline(xintercept = -1, linetype = "solid", alpha = 0.4) +
    stat_function(fun = dnorm, color = "#84CA72", size = 1.5) +
    ggtitle("Standard Normal Distribution") +
    xlab("x") +
    ylab("dnorm(x)") +
    theme_classic()
```

-   $P(-1 \le Z \le 1) = 68\%$

-   $P(-2 \le Z \le 2) = 95\%$

-   $P(-3 \le Z \le 3) = 99\%$

-   $P(-v \le Z \le v) = ?$ Check Z-table
:::
:::

## t-distribution

> A distribution of estimates sampling from normally distributed population when the sample size is small (**n \< 30**).

. . .

::: columns
::: {.column width="50%"}
-   Parameter $df$

-   For example, there is a population $\sim N(0, 1^2)$

    -   sample A (n = 7) -\> $\bar{x_A} = 0.5$

    -   sample B (n = 7) -\> $\bar{x_B} = 1.2$

    -   ...

    -   sample Z (n = 7) -\> $\bar{x_Z} = -0.7$
:::

::: {.column width="50%"}
-   The distribution of all the $\bar{x}$

```{r}
p <- data_frame(x = c(-4,4)) %>% 
    ggplot(aes(x = x)) 

p + stat_function(fun = dt, args = list(df = 6), color = "#84CA72", size = 1.5) +
    stat_function(fun = dnorm, color = "grey70", size = 1.5) +
    ggtitle("Central t-Distribution (df = 6, green) compared to z-Distribution (grey)") +
    xlab("x") +
    ylab("dt(x, 6) / dnorm(x)") +
    theme_classic()
```
:::
:::

. . .

**t-distribution is similar to standardized normal distribution (Z-distribution), especially when degree of freedom (or sample size) is high**

## F-distribution

> Use most notably in Analysis of Variance (ANOVA), which we will cover later in this course

$$
f(x) = \sqrt{\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x + d_2)^{d_1d_2}}}/xB(\frac{d_1}{2}\frac{d_2}{2})
$$

-   Parameter $df_1$ and $df_2$

```{r}
p <- data_frame(x = c(0, 5)) %>% 
    ggplot(aes(x = x)) 

p + stat_function(fun = df, args = list(df1 = 2, df2 = 40), 
                  color = "#84CA72", size = 1.5) +
    stat_function(fun = df, args = list(df1 = 5, df2 = 80), 
                  color = "grey70", size = 1.5) +
    stat_function(fun = df, args = list(df1 = 8, df2 = 120), 
                  color = "blue", size = 1.5, alpha = 0.7) +
    ggtitle("F-Distributions with df1 = 2, 5, 8 and df2 = 40, 80, 120") +
    xlab("x") +
    ylab("df(x, c(2, 5, 8), c(40, 80, 120))") +
    theme_classic()
```

## $\chi^2$ -distribution

> chi-squared test for goodness of fit of an observed distribution to a theoretical one

-   Parameter $df$

```{r}
p <- data_frame(x = c(0, 25)) %>% 
    ggplot(aes(x = x)) 

p + stat_function(fun = dchisq, args = list(df = 2), color = "#84CA72", size = 1.5) +
    stat_function(fun = dchisq, args = list(df = 4), color = "grey70", size = 1.5) +
    stat_function(fun = dchisq, args = list(df = 8), color = "blue", size = 1.5, alpha = 0.7) +
    stat_function(fun = dchisq, args = list(df = 14), color = "red", size = 1.5, alpha = 0.7) +
    ggtitle("Chi-Square Distributions with dfs = 2, 4, 8, and 14") +
    xlab("x") +
    ylab("dchisq(x, c(2, 4, 8, 14))") +
    theme_classic()
```

## What we learned

-   Random variable is the "thing" that you are interested in
-   Random variable can be continuous or categorical
-   A normal distribution is defined by $\mu$ and $\sigma$
-   A Z-distribution is a special case of normal distribution
-   A t-distribution is similar to Z-distribution, especially when degree of freedom is high

## Wrap up

### Before we meet again

-   Review the two lectures (important)

-   Review and practice with the R coding (also important)

-   Maybe use what we learned to explore your own data

### Next time

-   Tue. 12:30, virtual on zoom
-   Have a wonderful, relaxing, gull-like weekend
