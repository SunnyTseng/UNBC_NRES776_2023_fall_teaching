---
title: "NRES 776 Lecture 18"
subtitle: "Binomial regression"
author: Sunny Tseng
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    #css: styles.css
    #footer: <https://quarto.org>
    theme: [default, styles.scss]
    #smaller: true
    #scrollable: true
    incremental: true
    embed-resources: true
    width: 1200
    fontsize: 1.7em
editor: visual
---

```{r setup}
library(tidyverse)
library(here)
library(palmerpenguins)
library(lmtest)
library(interactions)
library(AER)
```

## Our schedule today

-   Announcement (0.5 min) - recording

-   

-   Wrap up (5 min)

## Overview of Binomial data

### Applications

-   Predict the winner of a sport game (team A or team B)

-   Predict animal behaviour (eat or not eat)

-   Evaluate business decisions (invest or not)

### Data requirement

-   Binary data (0 or 1)

-   Survival data (alive, dead)

-   Choice or behaviour (yes or no)

-   Result (pass or fail)

### In short

-   Use binomial regression when $y \sim Binomial(p)$

## Binomial distribution

::: columns
::: {.column width="50%"}
### Bernoulli distribution

-   Describe a variable which takes the value 1 with probability $p$ and the value 0 with probability $1-p$

-   The probability of front/back by flipping one coin.

-   $f(k;p) = p^k(1-p)^{1-k}$ for $k \in \{0, 1\}$

![](https://d138zd1ktt9iqe.cloudfront.net/media/seo_landing_files/bernoulli-distribution-graph-1634631289.png){fig-align="center" width="430"}
:::

::: {.column width="50%"}
### Binomial distribution

-   The number of successes in a sequence of $n$ independant experiments and each with its own Boolean valued outcome.

-   The probability of having $k$ coins facing up after tossing $n$ coins.

-   $f(k, n, p) = {n \choose x}p^k (1-p)^{1-k}$

![](https://datasciencelk.com/wp-content/uploads/2020/03/Binomial_Distribution-1536x1466.jpg.webp){fig-align="center" width="426"}
:::
:::

-   Bernoulli distribution is a special case of Binomial distribution when the draw equals to 1 ($n =1$).

## Overview of Binomial regression

1.  Systematic component

$$ \eta_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_p x_{pi} $$

2.  Link function $g()$ --\> most often **logit** (log odds), another one is called "probit"

$$ \eta_i = g(p_i) = logit(p_i) = log(\frac{p_i}{1-p_i}) $$

3.  Random component

$$ var(y_i) = n_ip_i(1-p_i) $$

## Overview of Binomial regression

### Equation for binomial regression

$$ logit(\mu_i) = logit(p_i) = log(\frac{p_i}{1-p_i})= \beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi} $$

### Equation for back transformation

$$
\mu_i = p_i = logit^{-1}(\beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi}) = \frac{1}{1+ exp(\beta_0 + \beta_1 x_{1i} + ...)}
$$

### Or, we often use odd to show the result

$$
Odd = \frac{p_i}{1-p_i} = exp(\beta_0 + \beta_1 x_{1i} + ... + \beta_p x_{pi})
$$

-   Odd is the probability that the event will occur divided by the probability that the event will not occur.

-   Odd increases as the probability increases.

![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*8ix_A7GUKH9AsZxouYg-uw.png){fig-align="center" width="288"}

## Overview of Binomial regression (con'd)

-   By default `family = binomial(link = "logit")` for this distribution.

```{=html}
<!-- -->
```
-   The variance for this distribution is `variance = "mu(1-mu)"`, and you cannot change it from the default.

```{r}
binomial_data <- read_delim(here("Lectures", "Lecture 16 - intro to GLM", "data", "isolation.txt"))  

bird_incidence <- binomial_data %>%   
  select(incidence, area)
```

```{r, echo = TRUE}
glm_binomial <- glm(formula = incidence ~ area, 
                    data = bird_incidence,                     
                    family = "binomial")  
glm_binomial %>% summary
```

## Long format vs wide format

::: columns
::: {.column width="50%"}
:::

::: {.column width="50%"}
:::
:::

## Research question (1 categorical x)

There is a investigation on how the tension (Low, Median, or High) on the number of warp breaks per loom. The "breaks" is the response variable which is a count of number of breaks. And the tension (L, M, H) is taken as the predictor variable.

::: columns
::: {.column width="50%"}
```{r}
breaks_overview <- warpbreaks %>%
  select(-wool) %>%
  mutate(round = rep(seq(1:18), 3)) %>%
  pivot_wider(names_from = tension, values_from = breaks)
breaks_overview
```
:::

::: {.column width="50%"}
Take a look at the group means

```{r, echo = TRUE}
warpbreaks %>%
  summarize(mean_breaks = mean(breaks), 
            .by = tension)
```
:::
:::

## Model formulation

$$
ln(\mu_i) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
$$

```{r, echo = TRUE}
breaks_poisson <- glm(formula = breaks ~ tension, data = warpbreaks, family = "poisson")

breaks_poisson %>% summary
```

## Coef. interpretation

-   For Low tension:

$$
ln(\mu_i) = \beta_0
$$

$$
\mu_i = exp(\beta_0) = exp(3.59) = 36.38
$$

. . .

-   For Median tension:

$$
ln(\mu_i) = \beta_0 + \beta_1
$$

$$
\mu_i = exp(\beta_0 + \beta_1) = exp(3.59) * exp(-0.32) = 26.38
$$

. . .

-   For High tension:

$$
ln(\mu_i) = \beta_0 + \beta_2
$$

$$
\mu_i = exp(\beta_0 + \beta_2) = exp(3.59) * exp(-0.51) = 21.66
$$

## Output interpretation

### Group mean prediction

-   The predicted group means are the same as the ones we calculated based on data

### R output

-   **Intercept (3.59)**: The number of breaks for reference level (low tension) is $exp(3.59)$

-   **tensionM (-0.32)**: The number of breaks for median tension is $exp(-0.32)$ **times** less than reference level (low tension)

-   **tensionH (-0.51)**: The number of breaks for high tension is $exp(-0.51)$ **times** less than reference level (low tension)

-   **Dispersion parameter (1)**: wonderful. Need to consider other methods if dispersion larger than 1 (over-dispersion) or smaller than 1 (under-dispersion)

-   **AIC (507.09)**: Can be used to compare the goodness of fit between models

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
breaks_poisson_null <- glm(formula = breaks ~ 1, 
                           data = warpbreaks,
                           family = "poisson")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Use `lrtest()` function in the `lmtest` package

```{r, echo = TRUE}
lrtest(breaks_poisson, breaks_poisson_null)
```

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
breaks_poisson_null <- glm(formula = breaks ~ 1,
                           data = warpbreaks, 
                           family = "poisson")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Or, Use `anova()` and specify `test = "Chisq"`

```{r, echo = TRUE}
anova(breaks_poisson, breaks_poisson_null, test = "Chisq")
```

## Predictor significance: Likelihood ratio test

-   Test whether adding one more variable `wool` (type of wool) could increase the model performance

$H_0$: The full model performance is the same as a reduced model (whichever model have fewer predictors)

$H_1$: The full model performance is significantly different comparing to a reduced model

. . .

```{r, echo = TRUE}
breaks_poisson_add <- glm(formula = breaks ~ tension + wool, 
                           data = warpbreaks,
                           family = "poisson")

lrtest(breaks_poisson, breaks_poisson_add)
```

## Model prediction

-   Use `predict()` and specify `type = "response"` to get back transformed $y_i$

```{r, echo = TRUE}
warpbreaks_p <- warpbreaks %>%
  mutate(breaks_p = predict(breaks_poisson_add, type = "response"))

warpbreaks_p
```

## Model prediction

-   Or, use `fitted()`, which provides back transformed $y_i$ by default

```{r, echo = TRUE}
warpbreaks_p <- warpbreaks %>%
  mutate(breaks_p = fitted(breaks_poisson_add))

warpbreaks_p
```

## Model visualization

-   Box plot: categorical predictor (y: count; x = tension)

```{r, echo = TRUE}
cat_plot(breaks_poisson_add, 
         pred = tension, 
         modx = wool, 
         geom = "line")
```

## Research question (1 continuous x)

A survey was done for 915 Candian PhD students to investigate the relationship between number of article published during the PhD and the number of mentors they have.

::: columns
::: {.column width="50%"}
```{r}
data("PhDPublications")
```

```{r}
phd_overview <- PhDPublications %>%   
  select(articles, mentor) 
  
phd_overview
```
:::

::: {.column width="50%"}
Take a look at the group means

```{r, echo = TRUE}
PhDPublications %>%   
  select(articles, mentor) %>%
  summarise(articles_mean = mean(articles), .by = mentor) %>%
  arrange(mentor) %>% 
  head()
```
:::
:::

## Model formulation

$$ ln(\mu_i) = \beta_0 + \beta_1 x_{1i} $$

```{r, echo = TRUE}
articles_poisson <- glm(formula = articles ~ mentor, 
                       data = PhDPublications, 
                       family = "poisson")

articles_poisson %>% summary()
```

## Coef. interpretation

-   For PhD students with no mentor:

$$ ln(\mu_i) = \beta_0 $$

$$ \mu_i = exp(\beta_0) = exp(0.25) = 1.29 $$

. . .

-   For PhD students having one mentor:

$$ ln(\mu_i) = \beta_0 + \beta_1 $$

$$ \mu_i = exp(\beta_0 + \beta_1) = exp(0.25) * exp(0.02) = 1.33 $$

. . .

-   For PhD students having more than one mentor(s):

$$ ln(\mu_i) = \beta_0 + \beta_1 x_{1i} $$

$$ \mu_i = exp(\beta_0 + \beta_1 x_{1i}) = exp(0.25) * exp(0.02 x_{1i}) $$

## Coef. interpretation

-   If $x$ is positive, then $exp(x)$ is larger than 1

-   If $x$ is positive, then $exp(x)$ is smaller than 1

![](images/exponential_function_two_to_x.png){fig-align="center"}

## Output interpretation

### Mean prediction

-   If the variable coefficient is greater than 0 -\> the counts gets higher as the variable increases, vice versa

### R output

-   **Intercept (0.25)**: The number of articles as baseline (no mentor) is $exp(0.25)$

-   **mentor (0.02)**: The number of articles with one mentor is $exp(0.02)$ **times** more than baseline

-   **Dispersion parameter (1)**: wonderful. Need to consider other methods if dispersion larger than 1 (over-dispersion) or smaller than 1 (under-dispersion)

-   **AIC (3341.3)**: Can be used to compare the goodness of fit between models

## Model goodness of fit: Likelihood ratio test

```{r, echo = TRUE}
articles_poisson_null <- glm(formula = articles ~ 1,
                             data = PhDPublications,
                             family = "poisson")
```

$H_0$: The model performance is the same as a null model (making predictions by chance)

$H_1$: The model performance is significantly different comparing to a null model

. . .

-   Use `lrtest()` function in the `lmtest` package

```{r, echo = TRUE}
lrtest(articles_poisson, articles_poisson_null)
```

## Model visualization

-   Smooth: continuous (y: count; x = continuous)

```{r, echo = TRUE}
ggplot(aes(x = mentor, y = articles), data = PhDPublications) + 
  geom_jitter(width = 0.5, height = 0.5) +
  geom_smooth(method = "glm", method.args = list(family = "poisson"))
```

## What we learned today

-   Count data follows Poisson distribution

-   Poisson uses log link as the most common link function

-   Coefficient interpretation on Poisson regression when there is 1 variable

-   Test goodness of fit using Likelihood Ratio Test

-   Poisson regression model prediction

-   Poisson regression model visualization

## Wrap up

### Before we meet again

-   Review intro to GLM and Poisson regression

-   Enjoy weekend!

### Next time

-   Next Tuesday 12:30 virtual lecture
